{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Techniques"
      ],
      "metadata": {
        "id": "F1vRhEHwEjFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "XyfPNFDcFa2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is about the below listed embedding techniques used to convert **text to numeric** values (vectors) so as to enable natural language processing in machine learning. Machine learning algorithms can't operate on raw text, we need to convert the text to numerical representation. This process is known as **embedding** the text.\n",
        "\n",
        "\n",
        "The list of embedding we will be exploring are:\n",
        "\n",
        "1- Bag of Words (BoW)\n",
        "\n",
        "2- TF - IDF (Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "3- Word2Vec\n",
        "\n",
        "4- GloVe\n",
        "\n",
        "5- FastText\n",
        "\n"
      ],
      "metadata": {
        "id": "fgwVbW8jEoLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "7M4LWCp1Fnp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW texhnique represents texts as frequency of words in the texts. It focuses in capturing the words in the input text and identifying their frequencies to create a vector representation of the input text. Hence we dont get any syntactical or semantic information of the text. Input text can be be anything from  few sentences to documents. Conceptually, we think of the whole document as a “bag” of words, rather than a sequence.\n",
        "\n",
        "The following steps are performed to generate a Bag of Words vector representation of text:\n",
        "\n",
        "- **tokenizing** strings: for instance by using white-spaces and punctuation as token separators and giving an integer id for each possible token.\n",
        "\n",
        "- **counting**: the occurrences of tokens in each document.\n",
        "\n",
        "Lets see Bag Of Words working using scikit-learn library.\n"
      ],
      "metadata": {
        "id": "5On4mD1tGh9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the CountVectorizer class from the feature_extraction.text module of the sklearn library (also known as scikit-learn)\n",
        "#CountVectorizer implements both tokenization and occurrence counting in a single class\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "AyQ86AXWhivp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CountVectorizer(*,\n",
        "#                input='content',\n",
        "#                encoding='utf-8',\n",
        "#                decode_error='strict',\n",
        "#                strip_accents=None,\n",
        "#                lowercase=True,\n",
        "#                preprocessor=None,\n",
        "#                tokenizer=None,\n",
        "#                stop_words=None,\n",
        "#                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "#                ngram_range=(1, 1),\n",
        "#                analyzer='word',\n",
        "#                max_df=1.0,\n",
        "#                min_df=1,\n",
        "#                max_features=None,\n",
        "#                vocabulary=None,\n",
        "#                binary=False,\n",
        "#                dtype=<class 'numpy.int64'>)"
      ],
      "metadata": {
        "id": "KLAR-VOCqTlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting the Bag of Words Vectorizer, with default configurations\n",
        "#token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b' - match words that are at least two characters long\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "UgJBMCVwn2RF",
        "outputId": "e7eb6b7a-59d8-434c-b749-c86a81e058ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use CountVectorizer to tokenize and count the word occurrences of a minimalistic corpus of text documents\n",
        "#corpus - list of strings\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "#Learns the corpus, creates vocabulary index and builds a document-term matrix (string to vocabulary matrix)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXVqiS2poEZO",
        "outputId": "bb1ff09e-7453-431b-95e0-043bad7e1ec1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 19 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of the CountVectorizer's fit_transform method is a document term matrix with 4 rows and 9 columns. 4 rows represent the 4 input documents in the corpus. 9 columns represent the tokenized words identified from the 4 documents."
      ],
      "metadata": {
        "id": "kN90uQQ5pVoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the tokens - this is the order of vocabulary index created\n",
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uTEPPi_qAZi",
        "outputId": "2acb7050-4d3e-4f11-a168-a39c00675722"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary index - a dictionary with token as key and their index as values\n",
        "#as we can see index is assigned on alphabetic order\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfIblZ4wt9dq",
        "outputId": "bb1911dc-cea9-46ce-c092-19e9361f99ca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 8,\n",
              " 'is': 3,\n",
              " 'the': 6,\n",
              " 'first': 2,\n",
              " 'document': 1,\n",
              " 'second': 5,\n",
              " 'and': 0,\n",
              " 'third': 7,\n",
              " 'one': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#document-term matrix\n",
        "#as per the vocabulary - 9 elements in each vector representation.\n",
        "#count of tokens present marked by their frequency in their respective index as per vocabulary index\n",
        "dtm_array = X.toarray()\n",
        "dtm_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TceZA7vLsxy6",
        "outputId": "f0087a0b-6d62-4268-e971-002f688645dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getting index of a term from vocabulary\n",
        "vectorizer.vocabulary_.get('first')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlQlDYst4Em",
        "outputId": "6ff8bd9f-2dbb-4362-8aa3-e8e3ebd88510"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieves the documents represented by the vocabulary terms from them\n",
        "vectorizer.inverse_transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kw1LiDt9bqG",
        "outputId": "ac461cc3-538c-4834-8245-0b6d54372007"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['this', 'is', 'the', 'first', 'document'], dtype='<U8'),\n",
              " array(['this', 'is', 'the', 'document', 'second'], dtype='<U8'),\n",
              " array(['the', 'and', 'third', 'one'], dtype='<U8'),\n",
              " array(['this', 'is', 'the', 'first', 'document'], dtype='<U8')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[0])\n",
        "print(corpus[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNn4PVfmvAI7",
        "outputId": "172f3cca-2b46-40f4-877c-beb7f4582a18"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first document.\n",
            "Is this the first document?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dtm_array[0])\n",
        "print(dtm_array[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gItXygWG0cFt",
        "outputId": "26e3787f-5ef7-4f33-fc31-4a5fbfd9b37c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 1 0 0 1 0 1]\n",
            "[0 1 1 1 0 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector representation for two different texts with same tokens in different order is the same** as Bag of Word embedding only looks at the freqencies of the tokens and not preserving token postions.\n",
        "\n",
        "**To resolve ambiguities encoded in local positioning patterns, we can use ngrams feature**. An n-gram is a contiguous sequence of n items (typically words) from a given text.\n",
        "\n",
        "The parameter ngram_range specifies the minimum and maximum range of continuous words to be extracted.\n",
        "\n",
        "ngram_range=(1, 1): Unigram, only single words are extracted\n",
        "\n",
        "ngram_range=(1, 2): Both unigrams and bigrams (pairs of consecutive words) are extracted.\n",
        "\n",
        "ngram_range=(1, 3): Unigrams, bigrams, and trigrams (sequences of three consecutive words) are extracted\n",
        "\n",
        "ngram_range=(2, 2): Only bigrams are extracted\n",
        "\n",
        "ngram_range allows to specify the range of n-grams (from unigrams to higher-order n-grams) that CountVectorizer should extract from the text, giving flexibility in capturing different levels of word combinations for text analysis.\n",
        "\n",
        "**Increasing the range (especially the upper limit) can significantly increase the number of features generated, which might lead to higher computational costs and memory usage.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Umherz6503oZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and Cons"
      ],
      "metadata": {
        "id": "9QiAR5VOF4Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While BoW is simple and interpretable representation, below disadvantages highlight its limitations in capturing certain aspects of language structure and semantics:\n",
        "\n",
        "- BoW ignores the order of words in the document, leading to a **loss of sequential information and context** making it less effective for tasks where word order is crucial, such as in natural language understanding.\n",
        "\n",
        "- BoW representations are **often sparse**, with many elements being zero resulting in increased memory requirements and computational inefficiency, especially when dealing with large datasets."
      ],
      "metadata": {
        "id": "EfHOsWE-DKTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications"
      ],
      "metadata": {
        "id": "pNs856fhDWqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bag of words model is typically used to embed documents in order to train a classifier (categorizes a document as belonging to one of multiple types).The mere presence and frequency of certain words is strongly indicative of what category the document belongs.\n",
        "\n",
        "Some common uses of the bag of words method include spam filtering and sentiment analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "azhvhxYnPI-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF - IDF"
      ],
      "metadata": {
        "id": "NFIv2m_wFsem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency - Inverse Document Frequency is known as TF-IDF. It is a numerical statistic that reflects the importance of a word in a document. It is commonly used in NLP to represent the relevance of a term to a document or a corpus of documents. The TF-IDF algorithm takes into account two main factors: the frequency of a word in a document (TF) and the frequency of the word across all documents in the corpus (IDF).\n",
        "\n",
        "**Term Frequency (TF)**\n",
        "\n",
        "**TF measures the frequency of a term within a document**. It is calculated as the ratio of the number of times a term occurs in a document to the total number of terms in that document. The resulting value is a number between 0 and 1. The goal is to emphasize words that are frequent within a document.\n",
        "\n",
        "TF(t,d) = count of t in d / number of words in d\n",
        "\n",
        "\n",
        "​**Inverse Document Frequency (IDF)**\n",
        "\n",
        "**IDF measures the importance of a term across a collection of documents**.It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. The resulting value is a number greater than or equal to 0. The goal is to diminish the weight of terms that occur very frequently in the document set and increases the weight of terms that rarely occur.\n",
        "\n",
        "With large corpus, say 100,000,000, the IDF value explodes. To avoid this, we take the log of IDF.\n",
        "\n",
        "During the query time, when a word that’s not in the vocabulary occurs, the DF will be 0. Since we can’t divide by 0, we smoothen the value by adding 1 to the denominator.\n",
        "\n",
        "And that’s the final formula.\n",
        "\n",
        "idf(t,D) = log(N/(df + 1))\n",
        "\n",
        "N:Total Number of Document\n",
        "df: Number of Documents with the term t\n",
        "\n",
        "IDF will be very low for the most occurring words, such as stop words like “is.” because those words are present in almost all of the documents.\n",
        "\n",
        "The **TF-IDF score** for a term in a document is obtained by multiplying its TF and IDF scores.\n",
        "\n",
        "TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)\n",
        "\n",
        "TF-IDF is a measure used to evaluate how important a word is to a document in a collection or corpus. The higher the TF-IDF score, the more important the term is in the document.\n",
        "\n",
        "Lets see working Tf-Idf vectorization using scikit-learn library."
      ],
      "metadata": {
        "id": "B7L15Hq7asB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the TfidfVectorizer class from the feature_extraction.text module of the sklearn library (also known as scikit-learn)\n",
        "#TfidfVectorizer class converts a collection of raw documents to a matrix of TF-IDF features\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "rldAZbXIigCF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TfidfVectorizer(*,\n",
        "#                input='content',\n",
        "#                encoding='utf-8',\n",
        "#                decode_error='strict',\n",
        "#                strip_accents=None,\n",
        "#                lowercase=True,\n",
        "#                preprocessor=None,\n",
        "#                tokenizer=None,\n",
        "#                analyzer='word',\n",
        "#                stop_words=None,\n",
        "#                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "#                ngram_range=(1, 1),\n",
        "#                max_df=1.0,\n",
        "#                min_df=1,\n",
        "#                max_features=None,\n",
        "#                vocabulary=None,\n",
        "#                binary=False,\n",
        "#                dtype=<class 'numpy.float64'>,\n",
        "#                norm='l2', use_idf=True,\n",
        "#                smooth_idf=True,\n",
        "#                sublinear_tf=False)"
      ],
      "metadata": {
        "id": "nPWYbCLNpo4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting the TF - IDF Vectorizer, with default configurations\n",
        "#token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b' - match words that are at least two characters long\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "KyZiGGGAt-zw",
        "outputId": "3f357167-3abe-4d42-b14d-c77167f501fd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Learns vocabulary and idf, return document-term matrix\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "X_tfidf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTRXQSrxuejJ",
        "outputId": "2fbf94e1-008e-450b-d00c-b45cbb2cecaa"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 19 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF values\n",
        "X_tfidf.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdh2Q1huvCfs",
        "outputId": "efc56d99-08ff-4f30-e5c4-4f3763d6cd6a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
              "        0.        , 0.35872874, 0.        , 0.43877674],\n",
              "       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n",
              "        0.85322574, 0.22262429, 0.        , 0.27230147],\n",
              "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
              "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
              "       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
              "        0.        , 0.35872874, 0.        , 0.43877674]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary - tokens with index\n",
        "tfidf_vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcL8LU1buusl",
        "outputId": "6ce29623-a3b4-4309-a3cd-218647df0d34"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 8,\n",
              " 'is': 3,\n",
              " 'the': 6,\n",
              " 'first': 2,\n",
              " 'document': 1,\n",
              " 'second': 5,\n",
              " 'and': 0,\n",
              " 'third': 7,\n",
              " 'one': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#identified tokens\n",
        "tfidf_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDnc11o_4DdV",
        "outputId": "b3811b3d-d380-4b39-840e-4b54725653ed"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IDF values of tokens\n",
        "tfidf_vectorizer.idf_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bk0EZoruObJ",
        "outputId": "0a26cbfe-6d93-4e1c-b39c-d0f5da5ec1ba"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.91629073, 1.22314355, 1.51082562, 1.22314355, 1.91629073,\n",
              "       1.91629073, 1.        , 1.91629073, 1.22314355])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nidf values:\\n')\n",
        "for ele1, ele2 in zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_):\n",
        "    if(ele1=='document'):\n",
        "      print(ele1, ':', ele2)\n",
        "    else:\n",
        "      print(ele1, '\\t:', ele2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9bLjlJx4nmv",
        "outputId": "02b32fa3-8fb2-4715-d6b0-1f67286fd6b7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "idf values:\n",
            "\n",
            "and \t: 1.916290731874155\n",
            "document : 1.2231435513142097\n",
            "first \t: 1.5108256237659907\n",
            "is \t: 1.2231435513142097\n",
            "one \t: 1.916290731874155\n",
            "second \t: 1.916290731874155\n",
            "the \t: 1.0\n",
            "third \t: 1.916290731874155\n",
            "this \t: 1.2231435513142097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and Cons"
      ],
      "metadata": {
        "id": "UQzwx8377C-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the advantages of using TF-IDF:\n",
        "\n",
        "- Measures relevance: TF-IDF helps to identify which terms are most relevant to a particular document.\n",
        "\n",
        "- Handles stop words: TF-IDF automatically down-weights common words that occur frequently in the text corpus (stop words) that do not carry much meaning or importance, making it a more accurate measure of term importance.\n",
        "\n",
        "Few of its limitations are:\n",
        "\n",
        "- Ignores the context: TF-IDF only considers the frequency of each term in a document, and does not take into account the context in which the term appears. This can lead to incorrect interpretations of the meaning of the document.\n",
        "\n",
        "- Assumes independence: TF-IDF assumes that the terms in a document are independent of each other. However, this is often not the case in natural language, where words are often related to each other in complex ways.\n",
        "\n",
        "- No concept of word order: TF-IDF treats words regardless of their order or position in the document. This can be problematic for certain applications, such as sentiment analysis, where word order can be crucial for determining the sentiment of a document."
      ],
      "metadata": {
        "id": "0nIEEGlqCNZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications"
      ],
      "metadata": {
        "id": "0Cn1Dm19DueN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some of the main applications of TF-IDF:\n",
        "\n",
        "- Search engines: TF-IDF is used in search engines to rank documents based on their relevance to a query. The TF-IDF score of a document is used to measure how well the document matches the search query.\n",
        "\n",
        "- Text classification: TF-IDF is used in text classification to identify the most important features in a document. The TF-IDF score of each term in the document is used to measure its relevance to the class.\n",
        "\n",
        "- Information extraction: TF-IDF is used in information extraction to identify the most important entities and concepts in a document. The TF-IDF score of each term is used to measure its importance in the document.\n",
        "\n",
        "- Keyword extraction: TF-IDF is used in keyword extraction to identify the most important keywords in a document. The TF-IDF score of each term is used to measure its importance in the document.\n",
        "\n",
        "- Recommender systems: TF-IDF is used in recommender systems to recommend items to users based on their preferences. The TF-IDF score of each item is used to measure its relevance to the user’s preferences.\n",
        "\n",
        "- Sentiment analysis: TF-IDF is used in sentiment analysis to identify the most important words in a document that contribute to the sentiment. The TF-IDF score of each word is used to measure its importance in the document.\n"
      ],
      "metadata": {
        "id": "KWGBQR3jD080"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "ck5sdpRWFzUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While BoW and TF-IDF were frequency based embedding technique which followed a deterministic approach, **Word2Vec is a prediction based embedding technique** that works with shallow neural networks.\n",
        "\n",
        "Word2Vec is able to preserve the context of the words in the text which was a drawback of both BoW and TF-IDF embeddings.  \n",
        "\n",
        "Word2Vec enables similar words to have similar dimensions and, consequently, helps bring context.\n",
        "\n",
        "There are **two neural embedding methods for Word2Vec**:\n",
        "\n",
        "- Continuous Bag of Words (CBOW)\n",
        "- Skip-gram\n",
        "\n"
      ],
      "metadata": {
        "id": "M8MBJh1eH2my"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continuous Bag of Words (CBOW):**\n",
        "\n",
        "- Its a **neural network architecture** used in Word2 Vec Model.\n",
        "- Predict a target word based on its context\n",
        "- It is a feedforward neural network with a single hidden layer\n",
        "- Input layer represents the context words\n",
        "- Output layer represents the target word\n",
        "- Hidden layer contains the learned continuous vector representations (word embeddings) of the input words\n",
        "- The weights between the input layer and the hidden layer are learned during training.\n",
        "- The dimensionality of the hidden layer represents the size of the word embeddings (the continuous vector space)."
      ],
      "metadata": {
        "id": "UIriztj0NtRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip-Gram:**\n",
        "\n",
        "- The model **learns distributed representations of words in a continuous vector space**.\n",
        "- It predict context words given a target word\n",
        "- It works just the opposite of CBOW\n",
        "\n",
        "\n",
        "On applying Skip-Gram model, we get trained vectors of each word after many iterations through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower dimensions. The vectors with similar meaning or semantic information are placed close to each other in space."
      ],
      "metadata": {
        "id": "Deu5WsCQPA_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec models can perform better with larger datasets as you achieve more meaningful word embeddings.\n",
        "\n",
        "CBOW might be preferred when training resources are limited, and capturing syntactic information is important.\n",
        "\n",
        "Skip-gram might be chosen when semantic relationships and the representation of rare words are crucial.\n",
        "\n",
        "While Skip-Gram frequently performs better for infrequent words, CBOW is faster and typically performs better with frequent words."
      ],
      "metadata": {
        "id": "XCewner4QTxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and Cons"
      ],
      "metadata": {
        "id": "3GqjK2C3RQJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLP, Word2Vec is a significant method for representing words as vectors in a continuous vector space. The advantages of the model are:\n",
        "\n",
        "- Semantic Representations: Word2Vec records the connections between words semantically. Words are represented in the vector space so that similar words are near to one another. This enables the model to interpret words according to their context within a particular corpus.\n",
        "\n",
        "- Distributional Semantics: Word2Vec generates vector representations that reflect semantic similarities by learning from the distributional patterns of words in a large corpus. Words with similar meanings are more likely to occur in similar contexts.\n",
        "\n",
        "- Vector Arithmetic: Word2Vec generates vector representations that have intriguing algebraic characteristics. Vector arithmetic, for instance, can be used to record word relationships. One well-known example is that the vector representation of “queen” could resemble the vector representation of “king” less “man” plus “woman.”\n",
        "\n",
        "- Transfer Learning: A variety of natural language processing tasks can be initiated with pre-trained Word2Vec models.\n",
        "\n",
        "- Scalability: Word2Vec can handle big corpora with ease and is scalable. Scalability like this is essential for training on large text datasets.\n",
        "\n",
        "\n",
        "Below are the limitations of Word2Vec:\n",
        "\n",
        "- Inability to handle unknown or out-of-vocabulary (OOV) words: If your model hasn’t encountered a word before, it will have no idea how to interpret it or how to build a vector for it.\n",
        "\n",
        "- No shared representations at sub-word levels: Word2vec represents every word as an independent vector, even though many words are morphologically similar, just like flawless or careless. This can also become a challenge in morphologically rich, and polysynthetic languages such as Arabic, German or Turkish."
      ],
      "metadata": {
        "id": "GXPnC58hRUL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications"
      ],
      "metadata": {
        "id": "VjPvciyIS3JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec embeddings used in a number of natural language processing (NLP) applications, such as machine translation, text classification, sentiment analysis, information retrieval and question answering. These applications are successful because of the capacity to capture semantic relationships."
      ],
      "metadata": {
        "id": "xCO0stPhS5Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe"
      ],
      "metadata": {
        "id": "e10DK-LwF4Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Global Vectors for Word Representation (GloVe)** is a powerful word embedding technique that captures the **semantic relationships** between words by considering their **co-occurrence probabilities within a corpus**.\n",
        "\n",
        "The key to GloVe’s effectiveness lies in the construction of a **word-context matrix and** the subsequent **factorization process**."
      ],
      "metadata": {
        "id": "yxvhplmCbKxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word-Context Matrix Formation**:\n",
        "\n",
        "The first step of GloVe involves creating a word-context matrix. This matrix is designed to represent the likelihood of a given word appearing near another across the entire corpus. Each cell in the matrix holds the co-occurrence count of how often words appear together in a certain context window."
      ],
      "metadata": {
        "id": "l9pZh68MbZq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Factorization for Word Vectors**:\n",
        "\n",
        "With the word-context matrix in place, next step in GloVe is matrix factorization. The objective here is to **decompose this high-dimensional matrix into two smaller matrices — one representing words and the other contexts**.\n",
        "\n",
        "Let’s denote W for words and C for contexts.\n",
        "\n",
        "The ideal scenario is when the dot product of W and CT (transpose of C) approximates the original word-context matrix:\n",
        "\n",
        "X≈W⋅CT\n",
        "\n",
        "Through iterative optimization, GloVe adjusts W and C to minimize the difference between X and W⋅CT. This process yields refined vector representations for each word, capturing the nuances of their co-occurrence patterns"
      ],
      "metadata": {
        "id": "vGwhxmbGb87i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Representations**:\n",
        "\n",
        "Once trained, GloVe provides each word with a dense vector that captures local context and global word usage patterns. **These vectors encode semantic and syntactic information**, revealing similarities and differences between words based on their overall usage in the corpus."
      ],
      "metadata": {
        "id": "li61p4bVc1lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and Cons"
      ],
      "metadata": {
        "id": "2GjDsICbdGyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "- Efficiently captures global statistics of the corpus.\n",
        "\n",
        "- Good at representing both semantic and syntactic relationships.\n",
        "\n",
        "- Effective in capturing word analogies.\n",
        "\n",
        "\n",
        "Cons:\n",
        "\n",
        "- Requires more memory for storing co-occurrence matrices.\n",
        "\n",
        "- Less effective with very small corpora.\n"
      ],
      "metadata": {
        "id": "0KKGG6FtdKTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications"
      ],
      "metadata": {
        "id": "KAIGhrOtfotm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Text Classification**: GloVe embeddings can be utilised as features in machine learning models for sentiment analysis, topic classification, spam detection, and other applications.\n",
        "\n",
        "- **Named Entity Recognition (NER)**: By capturing the semantic relationships between words and enhancing the model’s capacity to identify entities in text, GloVe embeddings can improve the performance of NER systems.\n",
        "\n",
        "- **Machine Translation**: GloVe embeddings can be used to represent words in the source and target languages in machine translation systems, which aim to translate text from one language to another, thereby enhancing the quality of the translation.\n",
        "\n",
        "- **Question Answering Systems**: To help models comprehend the context and relationships between words and produce more accurate answers, GloVe embeddings are used in question-answering tasks.\n",
        "\n",
        "- **Document Similarity and Clustering**: GloVe embeddings enable applications in information retrieval and document organization by measuring the semantic similarity between documents or grouping documents according to their content.\n",
        "\n",
        "- **Word Analogy Tasks**: In word analogy tasks, GloVe embeddings frequently yield good results. For instance, the generated vector for “king-man + woman” might resemble the “queen” vector, demonstrating the capacity to recognize semantic relationships.\n",
        "\n",
        "- **Semantic Search**: In semantic search applications, where retrieving documents or passages according to their semantic relevance to a user’s query is the aim, GloVe embeddings are helpful."
      ],
      "metadata": {
        "id": "dxlJqSKrfqjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText"
      ],
      "metadata": {
        "id": "42xwnTmoF9z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FastText** is an **advanced word embedding technique** developed by Facebook AI Research (FAIR) that **extends the Word2Vec model**. Unlike Word2Vec, **FastText not only considers whole words but also incorporates subword information — parts of words like n-grams**. This approach enables the **handling of morphologically rich languages** and captures information about word structure more effectively."
      ],
      "metadata": {
        "id": "VCErHQDegnhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subword Information**:\n",
        "\n",
        "FastText represents each word as a **bag of character n-grams in addition to the whole word** itself. This means that the word “apple” is represented by the word itself and its constituent n-grams like “ap”, “pp”, “pl”, “le”, etc. **This approach helps capture the meanings of shorter words and affords a better understanding of suffixes and prefixes**.\n",
        "\n"
      ],
      "metadata": {
        "id": "buf6-6-7iUDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**:\n",
        "\n",
        "Similar to Word2Vec, FastText can use either the CBOW or Skip-gram architecture. It incorporates the subword information during training. The neural network in FastText is trained to predict words (in CBOW) or context (in Skip-gram) not just based on the target words but also based on these n-grams."
      ],
      "metadata": {
        "id": "jHwVS38nidHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and Cons"
      ],
      "metadata": {
        "id": "nX5cH5b-iqXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Pros:\n",
        "\n",
        " Significant advantage of FastText is its ability to generate better word representations for **rare words or even words not seen during training**. By breaking down words into n-grams, FastText can construct meaningful representations for these words based on their subword units.\n",
        "\n",
        " Word2vec and GloVe both fail to provide any vector representation for words that are not in the model dictionary. This is a huge advantage of this method.\n",
        "\n",
        "- Better representation of rare words.\n",
        "\n",
        "- Capable of handling out-of-vocabulary words.\n",
        "\n",
        "- Richer word representations due to subword information.\n",
        "\n",
        "\n",
        "Cons:\n",
        "\n",
        "- Increased model size due to n-gram information.\n",
        "\n",
        "- Longer training times compared to Word2Vec.\n"
      ],
      "metadata": {
        "id": "nTX_UZWnivau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications"
      ],
      "metadata": {
        "id": "hwJ3gi-Jmw57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Text Classification and Categorisation**:\n",
        "\n",
        "**Spam filtering, topic categorisation, and content tagging** across various domains.\n",
        "\n",
        "- **Language Identification and Translation**\n",
        "\n",
        "The subword-level embeddings in fastText empower it to work with languages even in cases where only fragments or limited text samples are available. This proves beneficial in **language identification tasks**, aiding multilingual applications and facilitating language-specific processing. Additionally, fastText’s embeddings have been utilised to enhance **machine translation systems, improving the accuracy and performance of translation models**.\n",
        "\n",
        "- **Sentiment Analysis and Opinion Mining**\n",
        "\n",
        "In sentiment analysis, fastText’s robustness in capturing subtle linguistic nuances allows for more accurate sentiment classification. Its ability to understand and represent words based on their subword units enables a more profound comprehension of sentiment-laden expressions, contributing to more nuanced **opinion mining in social media analysis, product reviews, and customer feedback**.\n",
        "\n",
        "- **Entity Recognition and Tagging**\n",
        "\n",
        "Entity recognition involves identifying and classifying entities within a text, such as names of persons, organisations, locations, and more. fastText’s subword embeddings contribute to better handling of unseen or rare entities, improving the accuracy of entity recognition systems. This capability finds applications in **information extraction, search engines, and content analysis**."
      ],
      "metadata": {
        "id": "Zqkp_2drmzhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMARY"
      ],
      "metadata": {
        "id": "bi9oXP9Vjc0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing the Right Embedding Model**\n",
        "\n",
        "- Word2Vec: Use when semantic relationships are crucial, and you have a large dataset.\n",
        "\n",
        "- GloVe: Suitable for diverse datasets and when capturing global context is important.\n",
        "\n",
        "- FastText: Opt for morphologically rich languages or when handling out-of-vocabulary words is vital."
      ],
      "metadata": {
        "id": "rSpjFFjcjfPL"
      }
    }
  ]
}